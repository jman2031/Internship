{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da5daaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install Selenium Library\n",
    "!pip install selenium\n",
    "\n",
    "#then need webdriver. Go to about chrome. Download drivers from chromedriver.chromium.org. \n",
    "#Download whiever version of chrome you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a49b4373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the needed libraries\n",
    "\n",
    "import selenium\n",
    "import pandas as pd                                        # to create dataframe\n",
    "from selenium import webdriver                             #to open automated chrome window\n",
    "import warnings                                            #to ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time                                                #stop the search engine for few seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da7c04",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. \n",
    "You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "757c6655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to webdriver:\n",
    "#1. check version of your chrome.\n",
    "#2. go to the link- https://chromedriver.chromium.org/downloads\n",
    "#3. Choose the driver same as the chrome version.Download and extract it on your laptop\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\jyuth\\chrome driver\\chromedriver_win32\\chromedriver.exe\") #r is used to make it in a raw string\n",
    "\n",
    "#opening a website on a chrome window\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "#finding element for job search bar\n",
    "search_field_designation=driver.find_element_by_class_name(\"suggestor-input\") #job search bar\n",
    "search_field_designation.send_keys(\"Data Analyst\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1402fd95",
   "metadata": {},
   "source": [
    "#entering location Bangalore in location search bar\n",
    "search_field_location=driver.find_element_by_class_name(\"suggestor-input\") #job search bar\n",
    "search_field_location.send_keys(\"Bangalore\")\n",
    "\n",
    "#This will show the location in the job search bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d723055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering location Bangalore in location search bar\n",
    "search_field_location=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input\") #job search bar\n",
    "search_field_location.send_keys(\"Bangalore\")\n",
    "\n",
    "#clicking on search button\n",
    "search_button=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "52abcdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "#create 4 empty lists which needs to be extracted\n",
    "job=[]\n",
    "Location=[]\n",
    "company=[]\n",
    "experience=[]\n",
    "\n",
    "#find job title\n",
    "title_tags=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]') #locating web elements for the job title\n",
    "title_tags\n",
    "\n",
    "#we have all tags for the job title. We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in title_tags:            #iterating over web element of title\n",
    "    title=i.text            #extracting text from each web element\n",
    "    job.append(title)       #appending each extracted text in the empty list\n",
    "print(job)                    #printing data  \n",
    "\n",
    "#Extracting the html tags for the company\n",
    "company_tags=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "company_tags\n",
    "\n",
    "#we have all tags for the company. We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in company_tags:            #iterating over web element of title\n",
    "    Company=i.text                #extracting text from each web element\n",
    "    company.append(Company)       #appending each extracted text in the empty list\n",
    "print(company) \n",
    "\n",
    "#Extracting the html tags for the experience\n",
    "exp_tags=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]/span')\n",
    "exp_tags\n",
    "\n",
    "#we have all tags for the experience. We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in exp_tags:            #iterating over web element of title\n",
    "    exp=i.text                 #extracting text from each web element\n",
    "    experience.append(exp)      #appending each extracted text in the empty list\n",
    "print(experience)    \n",
    "\n",
    "#Extracting the html tags for the location\n",
    "locat_tags=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "locat_tags\n",
    "\n",
    "#we have all tags for the location. We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in locat_tags:            #iterating over web element of title\n",
    "    locat=i.text                #extracting text from each web element\n",
    "    Location.append(locat)      #appending each extracted text in the empty list\n",
    "Location\n",
    "\n",
    "#Check the length\n",
    "print(len(job),len(company),len(experience),len(Location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b707a111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business and Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru\\n(WFH during Covid)</td>\n",
       "      <td>CAREERDOST ENTERPRISE</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Pune, Gurgaon/Gurugram, Chennai, Bangalore/Ben...</td>\n",
       "      <td>Capco</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Gsn Games India</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jr . Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Armorblox</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru(Jayanagar)</td>\n",
       "      <td>G S E-COMMERCE PVT LTD</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hiring For Data Analyst with SAP ABAP &amp; BW - C...</td>\n",
       "      <td>Bangalore/Bengaluru\\n(WFH during Covid)</td>\n",
       "      <td>MILLION MINDS INFOTECH PRIVATE LIMITED</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Officer - Category Demand Management ( Data An...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Myntra</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Software Technologist II - Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Philips</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SAS Analyst / data Analyst / Business analyst ...</td>\n",
       "      <td>Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...</td>\n",
       "      <td>Leading US MNC into analytics</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sr Data Analyst</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Chennai, Bangalo...</td>\n",
       "      <td>TECHNODYSIS PRIVATE LIMITED</td>\n",
       "      <td>6-10 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_title  \\\n",
       "0                          Business and Data Analyst   \n",
       "1                                Senior Data Analyst   \n",
       "2                                Senior Data Analyst   \n",
       "3                                  Jr . Data Analyst   \n",
       "4                                       Data Analyst   \n",
       "5  Hiring For Data Analyst with SAP ABAP & BW - C...   \n",
       "6  Officer - Category Demand Management ( Data An...   \n",
       "7            Software Technologist II - Data Analyst   \n",
       "8  SAS Analyst / data Analyst / Business analyst ...   \n",
       "9                                    Sr Data Analyst   \n",
       "\n",
       "                                            Location  \\\n",
       "0            Bangalore/Bengaluru\\n(WFH during Covid)   \n",
       "1  Pune, Gurgaon/Gurugram, Chennai, Bangalore/Ben...   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                     Bangalore/Bengaluru(Jayanagar)   \n",
       "5            Bangalore/Bengaluru\\n(WFH during Covid)   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8  Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...   \n",
       "9  Hyderabad/Secunderabad, Pune, Chennai, Bangalo...   \n",
       "\n",
       "                             Company_name Experience  \n",
       "0                   CAREERDOST ENTERPRISE    0-5 Yrs  \n",
       "1                                   Capco   7-12 Yrs  \n",
       "2                         Gsn Games India    3-7 Yrs  \n",
       "3                               Armorblox    0-2 Yrs  \n",
       "4                  G S E-COMMERCE PVT LTD    4-7 Yrs  \n",
       "5  MILLION MINDS INFOTECH PRIVATE LIMITED   7-10 Yrs  \n",
       "6                                  Myntra    1-4 Yrs  \n",
       "7                                 Philips    5-8 Yrs  \n",
       "8           Leading US MNC into analytics    2-7 Yrs  \n",
       "9             TECHNODYSIS PRIVATE LIMITED   6-10 Yrs  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make DF with first 10 data\n",
    "D=pd.DataFrame({\"Job_title\":job,\"Location\":Location,\"Company_name\":company,\"Experience\":experience})\n",
    "D[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2dff33e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1679c",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. \n",
    "You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1835e853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"79fd2e29-821c-4e0e-a506-0abd4de322fe\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"7f9fe0ff-0d99-4215-8a33-e1b5720f0b63\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"b61492ad-404e-4d74-8743-4e3fbfcabada\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"f28ec18b-904b-4e61-bc37-fd17fb707ecb\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"de61ca5e-4267-47c9-81d3-dec79affb430\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"d5307b9b-b9a7-4170-ae3a-f3308d12e2cf\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"3859925d-bd37-4139-87a9-be28535aa751\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"d26bd984-cb54-4f9d-9ef8-797901898cf0\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"a86de608-95a8-4f80-be84-003bcb37e668\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"47af8e83-deb2-49bf-8e43-e67f9979b8d2\")>]\n",
      "['Senior Data Scientist', 'Senior Associate - Data Scientist', 'Data Scientist', 'Staff Data Scientist', 'Job Opening with Wipro For Data Scientist - SAS', 'Data Scientist', 'Senior Data Scientist -Python+ML(5-7 years)', 'Senior Data Scientist', 'Data Scientist', 'Senior Data Scientist (IN4)']\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"5135541c-47e0-4827-9b62-2e46ea94e187\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"1f5848b1-896c-4fd0-8a1a-165368ac6086\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"2b6005d2-921d-475a-b9de-daa8f51b5f58\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"3d8f9b6e-bdef-4935-86a9-59dc4cbf9b46\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"1243452c-d7c9-48d6-9348-e1130084d302\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"61cf54c7-7637-450e-bdec-1397d29ddfc5\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"67a4f0df-051a-4084-b68a-1d7f09f7a2e3\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"3a922917-83b4-4246-8bfe-67288897b43a\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"af68a20a-95b0-4c52-9ff5-bffb4cd8c36b\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"93cd9e18-2059-488d-9b9f-46409f54340b\")>]\n",
      "['Walmart', 'Affine', 'Applied Materials', 'Walmart', 'Wipro', 'CRED', 'Knowledge Foundry Business Solutions Pvt. Ltd.', 'Course5', 'Ashkom Media India Private Limited', 'Walmart']\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"4459ee82-df32-4e44-84b4-8e138b350972\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"15853730-5087-494f-95bc-ce6a68e2a163\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"7cefe1f9-f01c-4d2d-aea5-01b01464fae2\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"a098f87f-37f7-46f1-bc33-585b4f777d2d\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"8d96d614-b04b-4401-803e-bb2c3f87803f\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"f3963e04-de67-4da4-8300-91bd4d0f401f\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"39f9d463-e581-46f4-a022-a38e5132439c\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"acb00c87-35cb-4634-a316-bbea4de595de\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"50fe1b80-84fd-4c25-895c-900faf8942f4\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"cd3748ca-4122-4dc3-895e-b103a18a7d31\")>]\n",
      "['5-10 Yrs', '2-4 Yrs', '7-10 Yrs', '4-8 Yrs', '3-7 Yrs', '1-5 Yrs', '5-7 Yrs', '5-9 Yrs', '3-6 Yrs', '5-9 Yrs']\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"c25fcb19-6c62-480b-96bd-25264fe20992\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"4531186b-46c2-4d4a-a229-f5ea3faa1e7a\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"1892fc74-548e-44d4-83b6-d58328df5f5d\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"978b4008-2f18-4d29-8135-ef3bb068371b\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"0c55b8c4-81a8-48d3-8172-063e2147299f\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"d0a89629-f750-4499-92c2-0bc4541fe6f2\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"e05eed7c-dd0c-4413-8ac9-0cf4f45dc963\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"afee0426-e32f-4a85-9fcd-f475b287c71b\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"18fd78c9-9f20-4a1c-9841-0d5f9317bb38\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"5ea0080d0213af9eb7c63bb508558cdd\", element=\"f083da44-5c32-415f-9399-2d21dcc8886a\")>]\n",
      "['Bangalore/Bengaluru', 'Remote', 'Bangalore/Bengaluru', 'Bangalore/Bengaluru', 'Kolkata, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru', 'Bangalore/Bengaluru', 'Bangalore/Bengaluru', 'Bangalore/Bengaluru, Mumbai (All Areas)', 'Noida, Bangalore/Bengaluru', 'Bangalore/Bengaluru']\n",
      "20 20 20 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Associate - Data Scientist</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Affine</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Applied Materials</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Staff Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Job Opening with Wipro For Data Scientist - SAS</td>\n",
       "      <td>Kolkata, Hyderabad/Secunderabad, Pune, Chennai...</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>CRED</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Scientist -Python+ML(5-7 years)</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Knowledge Foundry Business Solutions Pvt. Ltd.</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai (All Areas)</td>\n",
       "      <td>Course5</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "      <td>Ashkom Media India Private Limited</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Scientist (IN4)</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Job_title  \\\n",
       "0                            Senior Data Scientist   \n",
       "1                Senior Associate - Data Scientist   \n",
       "2                                   Data Scientist   \n",
       "3                             Staff Data Scientist   \n",
       "4  Job Opening with Wipro For Data Scientist - SAS   \n",
       "5                                   Data Scientist   \n",
       "6      Senior Data Scientist -Python+ML(5-7 years)   \n",
       "7                            Senior Data Scientist   \n",
       "8                                   Data Scientist   \n",
       "9                      Senior Data Scientist (IN4)   \n",
       "\n",
       "                                            Location  \\\n",
       "0                                Bangalore/Bengaluru   \n",
       "1                                             Remote   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4  Kolkata, Hyderabad/Secunderabad, Pune, Chennai...   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7            Bangalore/Bengaluru, Mumbai (All Areas)   \n",
       "8                         Noida, Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                                          Company Experience  \n",
       "0                                         Walmart   5-10 Yrs  \n",
       "1                                          Affine    2-4 Yrs  \n",
       "2                               Applied Materials   7-10 Yrs  \n",
       "3                                         Walmart    4-8 Yrs  \n",
       "4                                           Wipro    3-7 Yrs  \n",
       "5                                            CRED    1-5 Yrs  \n",
       "6  Knowledge Foundry Business Solutions Pvt. Ltd.    5-7 Yrs  \n",
       "7                                         Course5    5-9 Yrs  \n",
       "8              Ashkom Media India Private Limited    3-6 Yrs  \n",
       "9                                         Walmart    5-9 Yrs  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#connect to webdriver:\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\jyuth\\chrome driver\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "#opening a website on a chrome window\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "#finding element for job search bar\n",
    "search_field_designation=driver.find_element_by_class_name(\"suggestor-input\") #job search bar\n",
    "search_field_designation.send_keys(\"Data Scientist\")\n",
    "\n",
    "#entering location Bangalore in location search bar\n",
    "search_field_location=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input\") #job search bar\n",
    "search_field_location.send_keys(\"Bangalore\")\n",
    "\n",
    "#clicking on search button\n",
    "search_button=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "search_button.click()\n",
    "\n",
    "#create 4 empty lists which needs to be extracted\n",
    "job=[]\n",
    "location=[]\n",
    "company=[]\n",
    "experience=[]\n",
    "\n",
    "#find job title\n",
    "title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\") #locating web elements for the job title\n",
    "print(title_tags[:10])\n",
    "\n",
    "#we have all tags for the job title. We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in title_tags:            #iterating over web element of title\n",
    "    title=i.text                #extracting text from each web element\n",
    "    job.append(title)           #appending each extracted text in the empty list\n",
    "print(job[:10])                 #printing top 10 data    \n",
    "\n",
    "#Extracting the html tags for the company\n",
    "company_tags=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "print(company_tags[:10])\n",
    "\n",
    "#we have all tags for the company. We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in company_tags:            #iterating over web element of title\n",
    "    Company=i.text                #extracting text from each web element\n",
    "    company.append(Company)       #appending each extracted text in the empty list\n",
    "print(company[:10])   \n",
    "\n",
    "#Extracting the html tags for the experience\n",
    "exp_tags=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]/span')\n",
    "print(exp_tags[:10])\n",
    "\n",
    "#we have all tags for the experience. We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in exp_tags:            #iterating over web element of title\n",
    "    exp=i.text            #extracting text from each web element\n",
    "    experience.append(exp)       #appending each extracted text in the empty list\n",
    "print(experience[:10])  \n",
    "\n",
    "#Extracting the html tags for the location\n",
    "loc_tags=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]/span')\n",
    "print(loc_tags[:10])\n",
    "\n",
    "#we have all tags for the location. We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in loc_tags:            #iterating over web element of title\n",
    "    loc=i.text                #extracting text from each web element\n",
    "    location.append(loc)    #appending each extracted text in the empty list\n",
    "print(location[:10])  \n",
    "\n",
    "#Check the length\n",
    "print(len(job),len(location),len(company),len(experience))\n",
    "\n",
    "#make data frame\n",
    "df=pd.DataFrame({'Job_title':job,'Location':location,'Company':company,'Experience':experience})\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9d66767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e0afa",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage naukri.com\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required. The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b30141eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"28216083-95e9-4131-aab1-3809e3e2fe1f\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"8b6033b2-3235-4393-aa3f-6b05fd84fd13\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"865d7899-13d9-4879-9311-6cc196be92b3\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"66ba607f-db71-4503-89ff-fd9ef6350901\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"3a6a63a4-9504-4b29-a741-ce8334ded388\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"fccaddb5-2030-49a4-8f08-936c7c2bc4be\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"08ab5644-1d3b-4a81-a21a-ea1e616c05d0\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"b64f9022-0352-480a-96bf-c9eeed531394\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"5e61b98a-f7b7-4bde-af6e-2c022a6702f9\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"09c2bc51-e338-46c0-a2aa-f9c5d2abd2f1\")>]\n",
      "['Data Scientist', 'Associate Data Scientist', 'Hiring For Senior Data Scientist-Noida', 'Hiring For Data Analyst and Data Scientist For Gurgaon Location', 'Opening For Data Scientist', 'Data Scientist/ Machine Learning, 2022 Passout Can also apply', 'Data Scientist (freelance)', 'Data Scientist', 'Data Scientist', 'Senior Data Scientist']\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"6053d038-2735-43af-9974-d80daa07803e\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"83e8200c-c6bc-4841-a8f0-6344cecd7401\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"28aee5c0-a1ba-4bb3-b600-fb054d5c5de7\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"e472eb9c-936b-438d-9ad0-db777dfe3971\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"e1843d99-1d5e-4223-896b-d9c09a913ba7\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"ec4f430b-e382-4b2b-b192-f2f78f571f87\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"ddb7d5ea-3fb7-421e-b5bf-58596ea975d7\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"799d7111-1ca4-4710-8e40-8deb1a93d4de\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"e60dc343-5c3f-4482-be95-bdc46daa7746\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"e7e529f3-6f47-4a9b-8fc8-c028e663f069\")>]\n",
      "['Noida, Bangalore/Bengaluru', 'Gurgaon/Gurugram', 'Noida, Greater Noida, Delhi / NCR', 'Noida, Gurgaon/Gurugram, Delhi / NCR', 'Gurgaon/Gurugram', 'Hyderabad/Secunderabad, Ahmedabad, Chennai, Bangalore/Bengaluru, Delhi / NCR, Mumbai (All Areas)', 'New Delhi, Delhi', 'Gurgaon, Bengaluru', 'New Delhi', 'Delhi']\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"4bfc09d2-151e-44a5-a5ab-9d5637628555\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"e5e4973a-363e-47a2-923f-f8faa3afe07f\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"118c4270-ec27-4ed1-b19b-2ee9d8178e80\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"dd340481-6062-4ae7-b92e-0d911247c762\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"29490471-4c14-4395-a261-60b45f4b087b\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"e34b3fc9-a2ca-4fa8-bdbf-2ba4f4ae28b6\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"5e10c6b9-2ca4-4b21-854b-bd7b3bef18ad\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"40ff75d0-2489-4aca-bd1c-2d0ce92fe783\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"13155d38-3fa0-42e3-8f1f-66e0bcf34d66\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"bc642a59-f62e-49de-959c-5e5e73b8186d\")>]\n",
      "['Ashkom Media India Private Limited', 'Optum', 'Lumiq.ai', 'Shadow Placements', 'Care Health Insurance', 'Creative Hands HR Consultancy', '2Coms', 'BlackBuck', 'Boston Consulting Group', 'iNICU']\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"c5f9a7db-cf9e-4190-a1fb-82fae7905415\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"75b2ca50-43cc-4050-b8fa-b742b96bbd03\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"c3ec82b0-f016-4c97-9747-c439ce293292\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"d4c686dc-240d-4a4e-9f90-58ab709a0e17\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"ec89483f-f627-4912-99ea-67c126c1e9bb\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"7d2eef76-403a-47c2-857a-691bdf750b91\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"131f39c0-db55-4d6c-bceb-fe6497e3f62b\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"822c9fb6-95ac-46ee-983a-c8d7bece509a\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"fa23a9e8-2e34-4589-b2f6-a9ad34c367bb\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"21f69935b91195376f671961968edd52\", element=\"1fd596ea-f17b-49b9-bed1-b83af105b34e\")>]\n",
      "['3-6 Yrs', '1-5 Yrs', '2-6 Yrs', '3-7 Yrs', '1-5 Yrs', '0-4 Yrs', '2-7 Yrs', '3-7 Yrs', '2-5 Yrs', '1-5 Yrs']\n"
     ]
    }
   ],
   "source": [
    "#connect to webdriver:\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\jyuth\\chrome driver\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "#opening a website on a chrome window\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "#finding element for job search bar\n",
    "search_job=driver.find_element_by_class_name(\"suggestor-input\") #job search bar\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "#clicking on search button\n",
    "search_button=driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "search_button.click()\n",
    "\n",
    "#Selecting location filter\n",
    "search_filter=driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[3]/label/p/span[1]')\n",
    "search_filter.click()\n",
    "\n",
    "#Selecting salary filter\n",
    "salary_filter=driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[5]/div[2]/div[2]/label/p/span[1]')\n",
    "salary_filter.click()\n",
    "\n",
    "#create Empty lists for scraping data\n",
    "job=[]\n",
    "location=[] \n",
    "company=[]\n",
    "experience=[]\n",
    "\n",
    "#find job title:\n",
    "title_tags=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]') #locating web elements for the job title\n",
    "print(title_tags[:10])\n",
    "\n",
    "#we have all tags for the job title. We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in title_tags:            #iterating over web element of title\n",
    "    title=i.text                #extracting text from each web element\n",
    "    job.append(title)           #appending each extracted text in the empty list\n",
    "print(job[:10])                 #printing top 10 data \n",
    "\n",
    "#Extracting the html tags for the location:\n",
    "loc_tags=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "print(loc_tags[:10])\n",
    "\n",
    "#we have all tags for the location. We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in loc_tags:            #iterating over web element of title\n",
    "    loc=i.text                #extracting text from each web element\n",
    "    location.append(loc)    #appending each extracted text in the empty list\n",
    "print(location[:10])  \n",
    "              \n",
    "#Extracting the html tags for the company:\n",
    "company_tags=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "print(company_tags[:10])\n",
    "\n",
    "#we have all tags for the company. We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in company_tags:            #iterating over web element of title\n",
    "    Company=i.text                #extracting text from each web element\n",
    "    company.append(Company)       #appending each extracted text in the empty list\n",
    "print(company[:10])   \n",
    "\n",
    "#Extracting the html tags for the experience:\n",
    "exp_tags=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "print(exp_tags[:10])\n",
    "\n",
    "#we have all tags for the experience. We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in exp_tags:            #iterating over web element of title\n",
    "    exp=i.text                #extracting text from each web element\n",
    "    experience.append(exp)    #appending each extracted text in the empty list\n",
    "print(experience[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "dd898a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "      <td>Ashkom Media India Private Limited</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Associate Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Optum</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hiring For Senior Data Scientist-Noida</td>\n",
       "      <td>Noida, Greater Noida, Delhi / NCR</td>\n",
       "      <td>Lumiq.ai</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hiring For Data Analyst and Data Scientist For...</td>\n",
       "      <td>Noida, Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "      <td>Shadow Placements</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Opening For Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Care Health Insurance</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist/ Machine Learning, 2022 Passout...</td>\n",
       "      <td>Hyderabad/Secunderabad, Ahmedabad, Chennai, Ba...</td>\n",
       "      <td>Creative Hands HR Consultancy</td>\n",
       "      <td>0-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist (freelance)</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "      <td>2Coms</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon, Bengaluru</td>\n",
       "      <td>BlackBuck</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>iNICU</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_title  \\\n",
       "0                                     Data Scientist   \n",
       "1                           Associate Data Scientist   \n",
       "2             Hiring For Senior Data Scientist-Noida   \n",
       "3  Hiring For Data Analyst and Data Scientist For...   \n",
       "4                         Opening For Data Scientist   \n",
       "5  Data Scientist/ Machine Learning, 2022 Passout...   \n",
       "6                         Data Scientist (freelance)   \n",
       "7                                     Data Scientist   \n",
       "8                                     Data Scientist   \n",
       "9                              Senior Data Scientist   \n",
       "\n",
       "                                            Location  \\\n",
       "0                         Noida, Bangalore/Bengaluru   \n",
       "1                                   Gurgaon/Gurugram   \n",
       "2                  Noida, Greater Noida, Delhi / NCR   \n",
       "3               Noida, Gurgaon/Gurugram, Delhi / NCR   \n",
       "4                                   Gurgaon/Gurugram   \n",
       "5  Hyderabad/Secunderabad, Ahmedabad, Chennai, Ba...   \n",
       "6                                   New Delhi, Delhi   \n",
       "7                                 Gurgaon, Bengaluru   \n",
       "8                                          New Delhi   \n",
       "9                                              Delhi   \n",
       "\n",
       "                              Company Experience  \n",
       "0  Ashkom Media India Private Limited    3-6 Yrs  \n",
       "1                               Optum    1-5 Yrs  \n",
       "2                            Lumiq.ai    2-6 Yrs  \n",
       "3                   Shadow Placements    3-7 Yrs  \n",
       "4               Care Health Insurance    1-5 Yrs  \n",
       "5       Creative Hands HR Consultancy    0-4 Yrs  \n",
       "6                               2Coms    2-7 Yrs  \n",
       "7                           BlackBuck    3-7 Yrs  \n",
       "8             Boston Consulting Group    2-5 Yrs  \n",
       "9                               iNICU    1-5 Yrs  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the length\n",
    "print(len(job),len(location),len(company),len(experience))\n",
    "\n",
    "#make data frame\n",
    "df=pd.DataFrame({'Job_title':job,'Location':location,'Company':company,'Experience':experience})\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3724914",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657d9eb",
   "metadata": {},
   "source": [
    "# Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2cbbad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to webdriver:\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\jyuth\\chrome driver\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "#opening a website on a chrome window\n",
    "driver.get('https://www.flipkart.com/')\n",
    "\n",
    "#finding element for sunglasses\n",
    "search=driver.find_element_by_class_name(\"_3704LK\") #in search bar\n",
    "search.send_keys(\"Sunglasses\")\n",
    "\n",
    "#clicking on search button\n",
    "search_button=driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button')\n",
    "search_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a92bd792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRYSTAL CART', 'Singco India', 'Fastrack', 'Fastrack', 'Elligator', 'PIRASO', 'Lee Topper', 'CRYSTAL CART', 'PIRASO', 'Fastrack', 'SUNBEE', 'Ray-Ban', 'ROZZETTA CRAFT', 'Fastrack', 'Fastrack', 'Ray-Ban', 'DEIXELS', 'SRPM', 'SHAAH COLLECTIONS', 'Ray-Ban', 'PIRASO', 'PIRASO', 'New Specs', 'Ray-Ban', 'SUNBEE', 'PIRASO', 'Mi', 'Ray-Ban', 'Fastrack', 'Fastrack', 'New Specs', 'Ray-Ban', 'NuVew', 'SRPM', 'ROYAL SON', 'VINCENT CHASE', 'PIRASO', 'Lee Topper', 'ROZZETTA CRAFT', 'Ray-Ban', 'Lee Topper', 'kingsunglasses', 'New Specs', 'ROZZETTA CRAFT', 'PIRASO', 'Fastrack', 'VINCENT CHASE', 'ROZZETTA CRAFT', 'hipe', 'Mi', 'Fastrack', 'BAJERO', 'ROZZETTA CRAFT', 'Fastrack', 'Fastrack', 'ROZZETTA CRAFT', 'Silver Kartz', 'elegante', 'Fastrack', 'Resist', 'ROZZETTA CRAFT', 'hipe', 'SRPM', 'ROYAL SON', 'Fastrack', 'Fastrack', 'kingsunglasses', 'povty', 'Singco India', 'New Specs', 'PHENOMENAL', 'ROYAL SON', 'New Specs', 'AISLIN', 'Mi', 'BAJERO', 'maxa', 'Poloport', 'HIPPON', 'VINCENT CHASE', 'NuVew', 'VINCENT CHASE', 'New Specs', 'BAJERO', 'ROYAL SON', 'ROYAL SON', 'Fastrack', 'CRYSTAL CART', 'Urbanic', 'ROYAL SON', 'AISLIN', 'RESIST EYEWEAR', 'Silver Kartz', 'GANSTA', 'Fastrack', 'ROYAL SON', 'Ray-Ban', 'NuVew', 'SUNBEE', 'VINCENT CHASE']\n",
      "['Mirrored Oval Sunglasses (62)', 'Gradient Retro Square Sunglasses (35)', 'UV Protection Aviator Sunglasses (55)', 'Gradient Round Sunglasses (52)', 'UV Protection Wayfarer Sunglasses (53)', 'Polarized, UV Protection Aviator Sunglasses (Free Size)', 'UV Protection Wayfarer Sunglasses (56)', 'Polarized, UV Protection Wrap-around Sunglasses (62)', 'UV Protection Aviator Sunglasses (58)', 'UV Protection Retro Square Sunglasses (58)', 'Polarized, UV Protection Sports Sunglasses (Free Size)', 'Gradient Retro Square Sunglasses (53)', 'UV Protection Wayfarer Sunglasses', 'UV Protection Round, Oval Sunglasses (52)', 'Polarized, Riding Glasses Sports, Wrap-around Sunglasse...', 'Mirrored Aviator Sunglasses (58)', 'Polarized Rectangular Sunglasses (62)', 'Polarized, UV Protection Retro Square Sunglasses (55)', 'UV Protection Cat-eye Sunglasses (60)', 'UV Protection, Gradient Retro Square Sunglasses (58)', 'Gradient Aviator Sunglasses (58)', 'Mirrored, UV Protection Retro Square Sunglasses (55)', 'UV Protection Aviator Sunglasses (Free Size)', 'UV Protection, Mirrored Wayfarer Sunglasses (57)', 'Toughened Glass Lens, UV Protection Rectangular Sunglas...', 'UV Protection, Riding Glasses Wayfarer Sunglasses (53)', 'UV Protection, Gradient Butterfly, Shield Sunglasses (6...', 'Polarized Rectangular Sunglasses (60)', 'UV Protection, Gradient Cat-eye Sunglasses (68)', 'UV Protection Sports Sunglasses (73)', 'UV Protection Rectangular Sunglasses (52)', 'UV Protection Wrap-around Sunglasses (Free Size)', 'UV Protection Wayfarer Sunglasses (Free Size)', 'UV Protection Oval Sunglasses (Free Size)', 'UV Protection, Polarized Aviator Sunglasses (60)', 'UV Protection Round Sunglasses (Free Size)', 'Mirrored, UV Protection Retro Square Sunglasses (Free S...', 'UV Protection Retro Square Sunglasses (Free Size)', 'UV Protection Aviator Sunglasses (57)', 'UV Protection Oval Sunglasses (59)', 'UV Protection, Night Vision, Riding Glasses Wayfarer Su...', 'Mirrored Aviator Sunglasses (55)', 'Mirrored, UV Protection Wayfarer, Wayfarer, Wayfarer, W...', 'Toughened Glass Lens, UV Protection Aviator Sunglasses ...', 'Polarized, UV Protection Sports Sunglasses (68)', 'UV Protection Aviator Sunglasses (60)', 'Others Retro Square Sunglasses (55)', 'Polarized, UV Protection Round Sunglasses (52)', 'Polarized, UV Protection Wrap-around, Rectangular Sungl...', 'Others Rectangular Sunglasses (56)', 'UV Protection, Mirrored Aviator Sunglasses (50)', 'Polarized, Gradient Cat-eye Sunglasses (57)', 'UV Protection Retro Square Sunglasses (50)', 'UV Protection Aviator Sunglasses (Free Size)', 'UV Protection, Mirrored, Night Vision, Riding Glasses S...', 'UV Protection, Riding Glasses Aviator, Wrap-around Sung...', 'Polarized, UV Protection Sports Sunglasses (68)', 'UV Protection Oval Sunglasses (Free Size)', 'UV Protection Aviator Sunglasses (57)', 'UV Protection Wayfarer Sunglasses (55)', 'by Lenskart UV Protection Rectangular Sunglasses (54)', 'Mirrored, Night Vision Rectangular Sunglasses (Free Siz...', 'Gradient Wayfarer, Wayfarer, Wayfarer Sunglasses (Free ...', 'UV Protection, Polarized, Mirrored Wayfarer Sunglasses ...', 'UV Protection Rectangular Sunglasses (64)', 'UV Protection, Mirrored, Riding Glasses Sports Sunglass...', 'Mirrored Aviator Sunglasses (Free Size)', 'UV Protection Butterfly, Retro Square Sunglasses (62)', 'by Lenskart UV Protection Round Sunglasses (48)', 'Gradient Wayfarer Sunglasses (Free Size)', 'UV Protection Retro Square Sunglasses (54)', 'UV Protection Round Sunglasses (55)', 'UV Protection Round Sunglasses (47)', 'UV Protection, Gradient Aviator Sunglasses (50)', 'Polarized, UV Protection Over-sized Sunglasses (61)', 'Polarized, UV Protection Wrap-around Sunglasses (62)', 'UV Protection Retro Square Sunglasses (Free Size)', 'UV Protection Aviator Sunglasses (50)', 'UV Protection Wayfarer, Shield Sunglasses (60)', 'UV Protection, Riding Glasses, Mirrored Aviator Sunglas...', 'UV Protection Retro Square Sunglasses (56)', 'UV Protection Rectangular Sunglasses (Free Size)', 'Mirrored Retro Square Sunglasses (53)', 'UV Protection Over-sized Sunglasses (57)', 'Toughened Glass Lens, UV Protection Round Sunglasses (5...', 'Polarized Aviator Sunglasses (53)', 'UV Protection, Mirrored Aviator Sunglasses (58)', 'Polarized Retro Square Sunglasses (51)', 'UV Protection, Polarized, Mirrored Aviator Sunglasses (...', 'UV Protection Wayfarer Sunglasses (55)', 'Toughened Glass Lens, UV Protection Aviator, Wrap-aroun...', 'UV Protection, Gradient Butterfly, Over-sized Sunglasse...', 'UV Protection Round, Shield Sunglasses (50)', 'UV Protection, Gradient Oval Sunglasses (60)', 'UV Protection, Gradient Butterfly, Shield Sunglasses (6...', 'Mirrored, UV Protection Round Sunglasses (53)', 'UV Protection Retro Square Sunglasses (56)', 'Toughened Glass Lens, UV Protection Wayfarer, Retro Squ...', 'UV Protection Rectangular Sunglasses (Free Size)', 'UV Protection Retro Square, Wayfarer, Spectacle Sungla...']\n",
      "['₹149', '₹10,431', '₹364', '₹1,214', '₹548', '₹229', '₹7,129', '₹426', '₹99', '₹825', '₹749', '₹255', '₹8,109', '₹803', '₹6,709', '₹998', '₹675', '₹178', '₹645', '₹379', '₹779', '₹99', '₹4,489', '₹1,145', '₹259', '₹339', '₹7,199', '₹1,234', '₹145', '₹675', '₹155', '₹403', '₹498', '₹597', '₹299', '₹498', '₹102', '₹189', '₹990', '₹498', '₹149', '₹10,431', '₹364', '₹1,214', '₹548', '₹229', '₹7,129', '₹426', '₹99', '₹825', '₹749', '₹255', '₹8,109', '₹803', '₹6,709', '₹998', '₹675', '₹178', '₹645', '₹379', '₹779', '₹99', '₹4,489', '₹1,145', '₹259', '₹339', '₹7,199', '₹1,234', '₹145', '₹675', '₹155', '₹403', '₹498', '₹597', '₹299', '₹498', '₹102', '₹189', '₹990', '₹498', '₹149', '₹10,431', '₹364', '₹1,214', '₹548', '₹229', '₹7,129', '₹426', '₹99', '₹825', '₹749', '₹255', '₹8,109', '₹803', '₹6,709', '₹998', '₹675', '₹178', '₹645', '₹379']\n",
      "['85% off', '10% off', '77% off', '84% off', '63% off', '80% off', '30% off', '78% off', '86% off', '77% off', '71% off', '69% off', '30% off', '73% off', '30% off', '73% off', '74% off', '82% off', '73% off', '86% off', '80% off', '80% off', '28% off', '69% off', '84% off', '83% off', '30% off', '58% off', '85% off', '72% off', '87% off', '72% off', '79% off', '70% off', '70% off', '67% off', '85% off', '85% off', '75% off', '67% off', '85% off', '10% off', '77% off', '84% off', '63% off', '80% off', '30% off', '78% off', '86% off', '77% off', '71% off', '69% off', '30% off', '73% off', '30% off', '73% off', '74% off', '82% off', '73% off', '86% off', '80% off', '80% off', '28% off', '69% off', '84% off', '83% off', '30% off', '58% off', '85% off', '72% off', '87% off', '72% off', '79% off', '70% off', '70% off', '67% off', '85% off', '85% off', '75% off', '67% off', '85% off', '10% off', '77% off', '84% off', '63% off', '80% off', '30% off', '78% off', '86% off', '77% off', '71% off', '69% off', '30% off', '73% off', '30% off', '73% off', '74% off', '82% off', '73% off', '86% off']\n",
      "100 100 100 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRYSTAL CART</td>\n",
       "      <td>Mirrored Oval Sunglasses (62)</td>\n",
       "      <td>₹149</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Singco India</td>\n",
       "      <td>Gradient Retro Square Sunglasses (35)</td>\n",
       "      <td>₹10,431</td>\n",
       "      <td>10% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Aviator Sunglasses (55)</td>\n",
       "      <td>₹364</td>\n",
       "      <td>77% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Gradient Round Sunglasses (52)</td>\n",
       "      <td>₹1,214</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (53)</td>\n",
       "      <td>₹548</td>\n",
       "      <td>63% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Mirrored, UV Protection Round Sunglasses (53)</td>\n",
       "      <td>₹998</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Ray-Ban</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (56)</td>\n",
       "      <td>₹675</td>\n",
       "      <td>74% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NuVew</td>\n",
       "      <td>Toughened Glass Lens, UV Protection Wayfarer, ...</td>\n",
       "      <td>₹178</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹645</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>UV Protection Retro Square, Wayfarer, Spectacl...</td>\n",
       "      <td>₹379</td>\n",
       "      <td>86% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Brand                                Product Description    Price  \\\n",
       "0    CRYSTAL CART                      Mirrored Oval Sunglasses (62)     ₹149   \n",
       "1    Singco India              Gradient Retro Square Sunglasses (35)  ₹10,431   \n",
       "2        Fastrack              UV Protection Aviator Sunglasses (55)     ₹364   \n",
       "3        Fastrack                     Gradient Round Sunglasses (52)   ₹1,214   \n",
       "4       Elligator             UV Protection Wayfarer Sunglasses (53)     ₹548   \n",
       "..            ...                                                ...      ...   \n",
       "95      ROYAL SON      Mirrored, UV Protection Round Sunglasses (53)     ₹998   \n",
       "96        Ray-Ban         UV Protection Retro Square Sunglasses (56)     ₹675   \n",
       "97          NuVew  Toughened Glass Lens, UV Protection Wayfarer, ...     ₹178   \n",
       "98         SUNBEE   UV Protection Rectangular Sunglasses (Free Size)     ₹645   \n",
       "99  VINCENT CHASE  UV Protection Retro Square, Wayfarer, Spectacl...     ₹379   \n",
       "\n",
       "   Discount  \n",
       "0   85% off  \n",
       "1   10% off  \n",
       "2   77% off  \n",
       "3   84% off  \n",
       "4   63% off  \n",
       "..      ...  \n",
       "95  73% off  \n",
       "96  74% off  \n",
       "97  82% off  \n",
       "98  73% off  \n",
       "99  86% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find Brand name, 100 data points:\n",
    "Brand=[]\n",
    "for i in range(0,3):       #running for the loop with range to run in this loop 3 times\n",
    "    brand_tags=driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]') #locating web element of title\n",
    "    for i in brand_tags:    #iterating over each web element of title\n",
    "        brand=i.text         #fetching text from the tags\n",
    "        Brand.append(brand)   #appending data(text) in empty list  \n",
    "     #code to click on next button\n",
    "    next_button=driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\")\n",
    "    next_button.click()\n",
    "    time.sleep(5)             #using the time function to pause the search engine for 5 sec\n",
    "Brand\n",
    "B=Brand[:100]\n",
    "print(B)\n",
    "\n",
    "#Extracting the html 100 tags for the product description:\n",
    "ProductDes=[] \n",
    "for i in range(0,4):                          #running for the loop with range to run in this loop 4 times\n",
    "    desc_tags=driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]')  #locating web element of title\n",
    "    for i in desc_tags:            #iterating over each web element of title\n",
    "        desc=i.text                #extracting text from each web element\n",
    "        ProductDes.append(desc)    #appending each extracted text in the empty list\n",
    "#code to click on next button\n",
    "    next_button=driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\")\n",
    "    next_button.click()\n",
    "    time.sleep(5)             #using the time function to pause the search engine for 5 sec\n",
    "ProductDes\n",
    "P=ProductDes[:100]\n",
    "print(P)\n",
    "              \n",
    "#Extracting the html 100 tags for the price:\n",
    "Price=[] \n",
    "for i in range(0,3):                          #running for the loop with range to run in this loop 3 times\n",
    "    price_tags=driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')  #locating web element of title\n",
    "    for i in price_tags:                       #iterating over each web element of title\n",
    "        price=i.text                #extracting text from each web element\n",
    "        Price.append(price)    #appending each extracted text in the empty list\n",
    "    #code to click on next button\n",
    "    next_button=driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\")\n",
    "    next_button.click()\n",
    "    time.sleep(5)  \n",
    "Price\n",
    "Pr=Price[:100]\n",
    "print(Pr)\n",
    "\n",
    "#Extracting the html 100 tags for the discount:\n",
    "Discount=[] \n",
    "for i in range(0,3):                          #running for the loop with range to run in this loop 3 times\n",
    "    discount_tags=driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')  #locating web element of title\n",
    "    for i in discount_tags:                       #iterating over each web element of title\n",
    "        discount=i.text                #extracting text from each web element\n",
    "        Discount.append(discount)    #appending each extracted text in the empty list\n",
    "    #code to click on next button\n",
    "    next_button=driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\")\n",
    "    next_button.click()\n",
    "    time.sleep(5)  \n",
    "Discount\n",
    "D=Discount[:100]\n",
    "print(D)\n",
    "\n",
    "#check Length\n",
    "print(len(B),len(Pr),len(P),len(D))\n",
    "\n",
    "#Make DataFrame\n",
    "DF=pd.DataFrame({\"Brand\":B,\"Product Description\":P,\"Price\":Pr,\"Discount\":D})\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b0f6453",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb51ce98",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: \n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power- adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC TSVZAXUHGREPBFGI&marketplace.\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b847fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Review Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.\\n\\nI’m am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4</td>\n",
       "      <td>Good choice</td>\n",
       "      <td>So far it’s been an AMAZING experience coming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>What a camera .....just awesome ..you can feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>i11 is worthy to buy, too much happy with the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ratings       Review Summary  \\\n",
       "0        5            Brilliant   \n",
       "1        5       Simply awesome   \n",
       "2        5  Best in the market!   \n",
       "3        5     Perfect product!   \n",
       "4        5            Fabulous!   \n",
       "..     ...                  ...   \n",
       "95       5    Worth every penny   \n",
       "96       5        Great product   \n",
       "97       4          Good choice   \n",
       "98       5   Highly recommended   \n",
       "99       5    Worth every penny   \n",
       "\n",
       "                                          Full Review  \n",
       "0   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "1   Really satisfied with the Product I received.....  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   Amazing phone with great cameras and better ba...  \n",
       "4   This is my first iOS phone. I am very happy wi...  \n",
       "..                                                ...  \n",
       "95  Previously I was using one plus 3t it was a gr...  \n",
       "96  Amazing Powerful and Durable Gadget.\\n\\nI’m am...  \n",
       "97  So far it’s been an AMAZING experience coming ...  \n",
       "98  What a camera .....just awesome ..you can feel...  \n",
       "99  i11 is worthy to buy, too much happy with the ...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#connect to webdriver:\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\jyuth\\chrome driver\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "#opening a website on a chrome window\n",
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power- adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC TSVZAXUHGREPBFGI&marketplace.')\n",
    "\n",
    "#find all reviews\n",
    "all_reviews=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[9]/div/div/div[5]/div/a/div/span\")\n",
    "all_reviews.click()\n",
    "\n",
    "#Extracting Ratings\n",
    "Ratings=[]\n",
    "for i in range(0,10):                          #running for the loop with range to run in this loop 10 times\n",
    "    rating_tags=driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]') #locating web element of title\n",
    "    for i in rating_tags:         #iterating over each web element of title\n",
    "        rating=i.text              #fetching text from the tags\n",
    "        Ratings.append(rating)      #appending data(text) in empty list\n",
    "\n",
    "#code to click on next button\n",
    "next_button=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]/span\")\n",
    "next_button.click()\n",
    "time.sleep(5)\n",
    "print(Ratings[:100])\n",
    "\n",
    "#Extracting the reviews\n",
    "Reviews=[]\n",
    "for i in range(0,10):                          #running for the loop with range to run in this loop 10 times\n",
    "    review_tags=driver.find_elements_by_xpath('//p[@class=\"_2-N8zT\"]') #locating web element of title\n",
    "    for i in review_tags:    #iterating over each web element of title\n",
    "        review=i.text         #fetching text from the tags\n",
    "        Reviews.append(review)   #appending data(text) in empty list\n",
    "\n",
    "#code to click on next button\n",
    "next_button=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]/span\")\n",
    "next_button.click()\n",
    "time.sleep(5)\n",
    "print(Reviews[:100])\n",
    "\n",
    "#For extracting the Review Summary\n",
    "Summary=[]\n",
    "for i in range(0,10):                          #running for the loop with range to run in this loop 10 times\n",
    "    summary_tags=driver.find_elements_by_xpath('//div[@class=\"t-ZTKy\"]') #locating web element of title\n",
    "    for i in summary_tags:    #iterating over each web element of title\n",
    "        summary=i.text         #fetching text from the tags\n",
    "        Summary.append(summary)   #appending data(text) in empty list\n",
    "\n",
    "#code to click on next button\n",
    "next_button=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]/span\")\n",
    "next_button.click()\n",
    "time.sleep(5)\n",
    "print(Summary[:100])\n",
    "\n",
    "#check Length\n",
    "print(len(Ratings),len(Reviews),len(Summary))\n",
    "\n",
    "#Make DataFrame\n",
    "dF=pd.DataFrame({\"Ratings\":Ratings,\"Review Summary\":Reviews,\"Full Review\":Summary})\n",
    "dF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92146323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to close automated chrome window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5ffad",
   "metadata": {},
   "source": [
    "# Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9a1f335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['54% off', '60% off', '82% off', '66% off', '77% off', '80% off', '52% off', '77% off', '64% off', '75% off', '52% off', '64% off', '52% off', '60% off', '51% off', '76% off', '40% off', '60% off', '50% off', '66% off', '56% off', '66% off', '64% off', '40% off', '50% off', '52% off', '50% off', '46% off', '50% off', '76% off', '73% off', '61% off', '27% off', '76% off', '50% off', '61% off', '62% off', '55% off', '50% off', '60% off', '80% off', '41% off', '76% off', '55% off', '37% off', '50% off', '46% off', '45% off', '50% off', '50% off', '55% off', '40% off', '52% off', '61% off', '31% off', '40% off', '46% off', '72% off', '51% off', '45% off', '50% off', '40% off', '52% off', '10% off', '50% off', '50% off', '50% off', '61% off', '15% off', '52% off', '40% off', '50% off', '47% off', '41% off', '38% off', '62% off', '52% off', '51% off', '40% off', '59% off', '73% off', '70% off', '66% off', '50% off', '50% off', '55% off', '51% off', '40% off', '57% off', '52% off', '44% off', '56% off', '50% off', '67% off', '52% off', '70% off', '40% off', '40% off', '59% off', '54% off']\n",
      "100 100 100 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jumpink</td>\n",
       "      <td>Kwik FIT casual sneaker shoes and partywear sh...</td>\n",
       "      <td>₹399</td>\n",
       "      <td>54% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KWIK FIT</td>\n",
       "      <td>STYLISH MENS BLACK SNEAKER Sneakers For Men</td>\n",
       "      <td>₹449</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>URBANBOX</td>\n",
       "      <td>Modern Trendy Sneakers Shoes Sneakers For Men</td>\n",
       "      <td>₹178</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>corsac</td>\n",
       "      <td>Kwik FIT casual sneaker shoes and partywear sh...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>66% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹295</td>\n",
       "      <td>77% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Krors</td>\n",
       "      <td>Stylish &amp; Trending Outdoor Walking Comfortable...</td>\n",
       "      <td>₹498</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Numenzo</td>\n",
       "      <td>Rebound Future Evo Sneakers For Men</td>\n",
       "      <td>₹599</td>\n",
       "      <td>40% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹499</td>\n",
       "      <td>40% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Men White Sneakers Sneakers For Men</td>\n",
       "      <td>₹539</td>\n",
       "      <td>59% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>BIG FOX</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹499</td>\n",
       "      <td>54% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Brand                                Product Description Price Discount\n",
       "0    Jumpink  Kwik FIT casual sneaker shoes and partywear sh...  ₹399  54% off\n",
       "1   KWIK FIT        STYLISH MENS BLACK SNEAKER Sneakers For Men  ₹449  60% off\n",
       "2   URBANBOX      Modern Trendy Sneakers Shoes Sneakers For Men  ₹178  82% off\n",
       "3     corsac  Kwik FIT casual sneaker shoes and partywear sh...  ₹499  66% off\n",
       "4     BRUTON                                   Sneakers For Men  ₹295  77% off\n",
       "..       ...                                                ...   ...      ...\n",
       "95     Krors  Stylish & Trending Outdoor Walking Comfortable...  ₹498  70% off\n",
       "96   Numenzo                Rebound Future Evo Sneakers For Men  ₹599  40% off\n",
       "97    Chevit                                   Sneakers For Men  ₹499  40% off\n",
       "98      PUMA                Men White Sneakers Sneakers For Men  ₹539  59% off\n",
       "99   BIG FOX                                   Sneakers For Men  ₹499  54% off\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#connect to webdriver:\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\jyuth\\chrome driver\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "#opening a website on a chrome window\n",
    "driver.get('https://www.flipkart.com/')\n",
    "\n",
    "#finding element for sneakers in search bar\n",
    "search_field_designation=driver.find_element_by_class_name(\"_3704LK\") #Sneakers in search bar\n",
    "search_field_designation.send_keys(\"Sneakers\")\n",
    "\n",
    "#clicking on search button\n",
    "search_button=driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button')\n",
    "search_button.click()\n",
    "\n",
    "#create 4 empty lists which needs to be extracted\n",
    "Brand=[]\n",
    "ProductD=[]\n",
    "Price=[]\n",
    "Discount=[]\n",
    "\n",
    "#find Brand name, 100 data points:\n",
    "for i in range(0,3):       #running for the loop with range to run in this loop 3 times\n",
    "    brand_tags=driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]') #locating web element of title\n",
    "    for i in brand_tags:    #iterating over each web element of title\n",
    "        brand=i.text         #fetching text from the tags\n",
    "        Brand.append(brand)   #appending data(text) in empty list  \n",
    "#code to click on next button\n",
    "    next_button=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\")\n",
    "    next_button.click()\n",
    "    time.sleep(5)             #using the time function to pause the search engine for 5 sec\n",
    "Brand\n",
    "B=Brand[:100]\n",
    "print(B)\n",
    "\n",
    "#Extracting the html 100 tags for the product description:\n",
    "for i in range(0,3):                          #running for the loop with range to run in this loop 3 times\n",
    "    desc_tags=driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]')  #locating web element of title\n",
    "    for i in desc_tags:            #iterating over each web element of title\n",
    "        desc=i.text                #extracting text from each web element\n",
    "        ProductD.append(desc)    #appending each extracted text in the empty list\n",
    "#code to click on next button\n",
    "    next_button=driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\")\n",
    "    next_button.click()\n",
    "    time.sleep(5)             #using the time function to pause the search engine for 5 sec\n",
    "ProductD\n",
    "P=ProductD[0:100]\n",
    "print(P)\n",
    "\n",
    "#Extracting the html 100 tags for the price:\n",
    "for i in range(0,3):                          #running for the loop with range to run in this loop 3 times\n",
    "    price_tags=driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')  #locating web element of title\n",
    "    for i in price_tags:                       #iterating over each web element of title\n",
    "        price=i.text                #extracting text from each web element\n",
    "        Price.append(price)    #appending each extracted text in the empty list\n",
    "    #code to click on next button\n",
    "    next_button=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\")\n",
    "    next_button.click()\n",
    "    time.sleep(5)  \n",
    "Price\n",
    "Pr=Price[:100]\n",
    "print(Pr)\n",
    "\n",
    "#Extracting the html 100 tags for the discount:\n",
    "for i in range(0,3):                          #running for the loop with range to run in this loop 3 times\n",
    "    discount_tags=driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')  #locating web element of title\n",
    "    for i in discount_tags:                       #iterating over each web element of title\n",
    "        discount=i.text                #extracting text from each web element\n",
    "        Discount.append(discount)    #appending each extracted text in the empty list\n",
    "#code to click on next button\n",
    "    next_button=driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\")\n",
    "    next_button.click()\n",
    "    time.sleep(5)  \n",
    "Discount\n",
    "D=Discount[:100]\n",
    "print(D)\n",
    "\n",
    "#check Length\n",
    "print(len(B),len(Pr),len(P),len(D))\n",
    "\n",
    "#Make DataFrame\n",
    "DF=pd.DataFrame({\"Brand\":B,\"Product Description\":P,\"Price\":Pr,\"Discount\":D})\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0198f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ac0cb",
   "metadata": {},
   "source": [
    "# Q7: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 7149 to Rs. 14099 ” , Color filter to “Black”\n",
    "Scrap first 100 data - which includes brand, description, price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "efbc11f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALDO', 'ALDO', 'Skechers', 'UNDER ARMOUR', 'Nike', 'Puma', 'UNDER ARMOUR', 'UNDER ARMOUR', 'Skechers', 'ALDO', 'ALDO', 'Hush Puppies', 'Skechers', 'ALDO', 'UNDER ARMOUR', 'ALDO', 'PUMA Motorsport', 'Puma', 'DAVINCHI', 'Nike', 'PUMA Motorsport', 'Puma', 'Puma', 'Hush Puppies', 'UNDER ARMOUR', 'PUMA Motorsport', 'UNDER ARMOUR', 'Puma', 'ADIDAS Originals', 'Reebok', 'PUMA Motorsport', 'UNDER ARMOUR', 'Puma', 'Puma', 'Geox', 'UNDER ARMOUR', 'Puma', 'PUMA Motorsport', 'Tommy Hilfiger', 'PUMA Motorsport', 'Puma', 'Nike', 'Puma', 'ASICS', 'Puma', 'Geox', 'PUMA Motorsport', 'Louis Philippe', 'PUMA Motorsport', 'PUMA Motorsport', 'ALDO', 'ALDO', 'Skechers', 'UNDER ARMOUR', 'Nike', 'Puma', 'UNDER ARMOUR', 'UNDER ARMOUR', 'Skechers', 'ALDO', 'ALDO', 'Hush Puppies', 'Skechers', 'ALDO', 'UNDER ARMOUR', 'ALDO', 'PUMA Motorsport', 'Puma', 'DAVINCHI', 'Nike', 'PUMA Motorsport', 'Puma', 'Puma', 'Hush Puppies', 'UNDER ARMOUR', 'PUMA Motorsport', 'UNDER ARMOUR', 'Puma', 'ADIDAS Originals', 'Reebok', 'PUMA Motorsport', 'UNDER ARMOUR', 'Puma', 'Puma', 'Geox', 'UNDER ARMOUR', 'Puma', 'PUMA Motorsport', 'Tommy Hilfiger', 'PUMA Motorsport', 'Puma', 'Nike', 'Puma', 'ASICS', 'Puma', 'Geox', 'PUMA Motorsport', 'Louis Philippe', 'PUMA Motorsport', 'PUMA Motorsport']\n",
      "['Men Textured Slip-On Sneakers', 'Men Sneakers', 'Men Max Cushioning Running', 'Men Vantage 2 Running Shoes', 'Women React MR 3 Running Shoes', 'Men Colourblocked RS-Z Core', 'Men UA Charged Breeze Training', 'Men Charged Rouge 3 Run Shoes', 'Men MAX CUSHIONING ELITE-LUCID', 'Men Woven Design Sneakers', 'Men Leather Driving Shoes', 'Men Solid Leather Formal Slip-Ons', 'Men Sky Vault Walking Shoes', 'Men Textured Sneakers', 'Women Breeze Running Shoes', 'Men Leather Loafers', 'Unisex Ferrari IONSpeed', 'Eternity Nitro Running Shoes', 'Men Solid Leather Formal Loafers', 'Men Air Max Dawn Sneakers', 'Unisex Mercedes F1 Sneakers', 'Men Deviate Nitro Shoes', 'Unisex Leather Sneakers', 'Men Formal Derbys', 'Men UA Charged Vantage 2 Run', 'Unisex Mercedes F1 R-Cat', 'Men UA TriBase Reign4 Training', 'Men M Nitro Running Shoess', 'Men Solid Ozweego Sneakers', 'Men Zig Dynamica 2.0 Running', 'Men MAPF1 X-Ray Speed Sneakers', 'Men Charged Breeze Running', 'Men Velocity Nitro Running', 'Men Running Shoes', 'Men Textured Leather Driving Shoes', 'Women Charged Vantage 2 Run', 'Unisex Leather Trainers', 'Men BMW MMS R-Cat Sneakers', 'Men Leather Sneakers', 'Unisex Leather Sneakers', 'Unisex KING Pro 21 Football', 'Men AIR MAX PRE-DAY Sneakers', 'Women Running Sports Shoes', 'Men Running Shoes', 'Men D Nitro WTR Running Shoes', 'Men Textured Leather Driving Shoes', 'Men ZenonSpeed Sneakers', 'Men Solid Leather Formal Loafers', 'Unisex Porsche Legacy Sneakers', 'Ferrari R-Cat Machina Sneakers', 'Men Textured Slip-On Sneakers', 'Men Sneakers', 'Men Max Cushioning Running', 'Men Vantage 2 Running Shoes', 'Women React MR 3 Running Shoes', 'Men Colourblocked RS-Z Core', 'Men UA Charged Breeze Training', 'Men Charged Rouge 3 Run Shoes', 'Men MAX CUSHIONING ELITE-LUCID', 'Men Woven Design Sneakers', 'Men Leather Driving Shoes', 'Men Solid Leather Formal Slip-Ons', 'Men Sky Vault Walking Shoes', 'Men Textured Sneakers', 'Women Breeze Running Shoes', 'Men Leather Loafers', 'Unisex Ferrari IONSpeed', 'Eternity Nitro Running Shoes', 'Men Solid Leather Formal Loafers', 'Men Air Max Dawn Sneakers', 'Unisex Mercedes F1 Sneakers', 'Men Deviate Nitro Shoes', 'Unisex Leather Sneakers', 'Men Formal Derbys', 'Men UA Charged Vantage 2 Run', 'Unisex Mercedes F1 R-Cat', 'Men UA TriBase Reign4 Training', 'Men M Nitro Running Shoess', 'Men Solid Ozweego Sneakers', 'Men Zig Dynamica 2.0 Running', 'Men MAPF1 X-Ray Speed Sneakers', 'Men Charged Breeze Running', 'Men Velocity Nitro Running', 'Men Running Shoes', 'Men Textured Leather Driving Shoes', 'Women Charged Vantage 2 Run', 'Unisex Leather Trainers', 'Men BMW MMS R-Cat Sneakers', 'Men Leather Sneakers', 'Unisex Leather Sneakers', 'Unisex KING Pro 21 Football', 'Men AIR MAX PRE-DAY Sneakers', 'Women Running Sports Shoes', 'Men Running Shoes', 'Men D Nitro WTR Running Shoes', 'Men Textured Leather Driving Shoes', 'Men ZenonSpeed Sneakers', 'Men Solid Leather Formal Loafers', 'Unisex Porsche Legacy Sneakers', 'Ferrari R-Cat Machina Sneakers']\n",
      "['Rs. 12999', 'Rs. 9999', 'Rs. 7199Rs. 8999(20% OFF)', 'Rs. 7999', 'Rs. 8920Rs. 10495(15% OFF)', 'Rs. 8499Rs. 9999(15% OFF)', 'Rs. 8999', 'Rs. 7999', 'Rs. 7599Rs. 9499(20% OFF)', 'Rs. 13999', 'Rs. 12999', 'Rs. 7649Rs. 8999(15% OFF)', 'Rs. 7199Rs. 8999(20% OFF)', 'Rs. 10999', 'Rs. 8999', 'Rs. 12999', 'Rs. 7649Rs. 8999(15% OFF)', 'Rs. 10399Rs. 12999(20% OFF)', 'Rs. 8490', 'Rs. 9295', 'Rs. 7999', 'Rs. 11999Rs. 14999(20% OFF)', 'Rs. 7999Rs. 9999(20% OFF)', 'Rs. 8499Rs. 9999(15% OFF)', 'Rs. 7999', 'Rs. 7499', 'Rs. 11999', 'Rs. 10399Rs. 12999(20% OFF)', 'Rs. 9349Rs. 10999(15% OFF)', 'Rs. 7999', 'Rs. 7649Rs. 8999(15% OFF)', 'Rs. 8999', 'Rs. 9599Rs. 11999(20% OFF)', 'Rs. 7999Rs. 9999(20% OFF)', 'Rs. 10490', 'Rs. 7999', 'Rs. 7999Rs. 9999(20% OFF)', 'Rs. 7499', 'Rs. 7309Rs. 8599(15% OFF)', 'Rs. 7999', 'Rs. 7999Rs. 9999(20% OFF)', 'Rs. 13995', 'Rs. 11999Rs. 14999(20% OFF)', 'Rs. 7999', 'Rs. 12799Rs. 15999(20% OFF)', 'Rs. 10990', 'Rs. 8999', 'Rs. 9999', 'Rs. 7999', 'Rs. 7499', 'Rs. 12999', 'Rs. 9999', 'Rs. 7199Rs. 8999(20% OFF)', 'Rs. 7999', 'Rs. 8920Rs. 10495(15% OFF)', 'Rs. 8499Rs. 9999(15% OFF)', 'Rs. 8999', 'Rs. 7999', 'Rs. 7599Rs. 9499(20% OFF)', 'Rs. 13999', 'Rs. 12999', 'Rs. 7649Rs. 8999(15% OFF)', 'Rs. 7199Rs. 8999(20% OFF)', 'Rs. 10999', 'Rs. 8999', 'Rs. 12999', 'Rs. 7649Rs. 8999(15% OFF)', 'Rs. 10399Rs. 12999(20% OFF)', 'Rs. 8490', 'Rs. 9295', 'Rs. 7999', 'Rs. 11999Rs. 14999(20% OFF)', 'Rs. 7999Rs. 9999(20% OFF)', 'Rs. 8499Rs. 9999(15% OFF)', 'Rs. 7999', 'Rs. 7499', 'Rs. 11999', 'Rs. 10399Rs. 12999(20% OFF)', 'Rs. 9349Rs. 10999(15% OFF)', 'Rs. 7999', 'Rs. 7649Rs. 8999(15% OFF)', 'Rs. 8999', 'Rs. 9599Rs. 11999(20% OFF)', 'Rs. 7999Rs. 9999(20% OFF)', 'Rs. 10490', 'Rs. 7999', 'Rs. 7999Rs. 9999(20% OFF)', 'Rs. 7499', 'Rs. 7309Rs. 8599(15% OFF)', 'Rs. 7999', 'Rs. 7999Rs. 9999(20% OFF)', 'Rs. 13995', 'Rs. 11999Rs. 14999(20% OFF)', 'Rs. 7999', 'Rs. 12799Rs. 15999(20% OFF)', 'Rs. 10990', 'Rs. 8999', 'Rs. 9999', 'Rs. 7999', 'Rs. 7499']\n",
      "100 100 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Textured Slip-On Sneakers</td>\n",
       "      <td>Rs. 12999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Sneakers</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Skechers</td>\n",
       "      <td>Men Max Cushioning Running</td>\n",
       "      <td>Rs. 7199Rs. 8999(20% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UNDER ARMOUR</td>\n",
       "      <td>Men Vantage 2 Running Shoes</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Women React MR 3 Running Shoes</td>\n",
       "      <td>Rs. 8920Rs. 10495(15% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Textured Leather Driving Shoes</td>\n",
       "      <td>Rs. 10990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>PUMA Motorsport</td>\n",
       "      <td>Men ZenonSpeed Sneakers</td>\n",
       "      <td>Rs. 8999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Louis Philippe</td>\n",
       "      <td>Men Solid Leather Formal Loafers</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>PUMA Motorsport</td>\n",
       "      <td>Unisex Porsche Legacy Sneakers</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>PUMA Motorsport</td>\n",
       "      <td>Ferrari R-Cat Machina Sneakers</td>\n",
       "      <td>Rs. 7499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Brand                 Product Description  \\\n",
       "0              ALDO       Men Textured Slip-On Sneakers   \n",
       "1              ALDO                        Men Sneakers   \n",
       "2          Skechers          Men Max Cushioning Running   \n",
       "3      UNDER ARMOUR         Men Vantage 2 Running Shoes   \n",
       "4              Nike      Women React MR 3 Running Shoes   \n",
       "..              ...                                 ...   \n",
       "95             Geox  Men Textured Leather Driving Shoes   \n",
       "96  PUMA Motorsport             Men ZenonSpeed Sneakers   \n",
       "97   Louis Philippe    Men Solid Leather Formal Loafers   \n",
       "98  PUMA Motorsport      Unisex Porsche Legacy Sneakers   \n",
       "99  PUMA Motorsport      Ferrari R-Cat Machina Sneakers   \n",
       "\n",
       "                         Price  \n",
       "0                    Rs. 12999  \n",
       "1                     Rs. 9999  \n",
       "2    Rs. 7199Rs. 8999(20% OFF)  \n",
       "3                     Rs. 7999  \n",
       "4   Rs. 8920Rs. 10495(15% OFF)  \n",
       "..                         ...  \n",
       "95                   Rs. 10990  \n",
       "96                    Rs. 8999  \n",
       "97                    Rs. 9999  \n",
       "98                    Rs. 7999  \n",
       "99                    Rs. 7499  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#connect to webdriver:\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\jyuth\\chrome driver\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "#opening a website on a chrome window\n",
    "driver.get('https://www.myntra.com/shoes')\n",
    "\n",
    "#set price filter\n",
    "price_filter=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label\")\n",
    "price_filter.click()\n",
    "\n",
    "#create 3 empty lists which needs to be extracted\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "\n",
    "#find Brand name, 100 data points:\n",
    "for i in range(0,2):       #running for the loop with range to run in this loop 2 times\n",
    "    brand_tags=driver.find_elements_by_xpath('//h3[@class=\"product-brand\"]') #locating web element of title\n",
    "    for i in brand_tags:    #iterating over each web element of title\n",
    "        b=i.text            #fetching text from the tags\n",
    "        brand.append(b)      #appending data(text) in empty list  \n",
    "#code to click on next button\n",
    "    next_button=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[2]/div/div[2]/section/div[2]/ul/li[2]/a\")\n",
    "    next_button.click()\n",
    "    time.sleep(5)             #using the time function to pause the search engine for 5 sec\n",
    "brand\n",
    "B=brand[:100]\n",
    "print(B)\n",
    "\n",
    "#Extracting the html 100 tags for the product description:\n",
    "for i in range(0,2):                          #running for the loop with range to run in this loop 2 times\n",
    "    desc_tags=driver.find_elements_by_xpath('//h4[@class=\"product-product\"]')  #locating web element of title\n",
    "    for i in desc_tags:            #iterating over each web element of title\n",
    "        desc=i.text                #extracting text from each web element\n",
    "        description.append(desc)    #appending each extracted text in the empty list\n",
    "#code to click on next button\n",
    "    next_button=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[2]/div/div[2]/section/div[2]/ul/li[2]/a\")\n",
    "    next_button.click()\n",
    "    time.sleep(5)             #using the time function to pause the search engine for 5 sec\n",
    "description\n",
    "P=description[:100]\n",
    "print(P)\n",
    "\n",
    "#Extracting the html 100 tags for the price:\n",
    "for i in range(0,5):                          #running for the loop with range to run in this loop 5 times\n",
    "    price_tags=driver.find_elements_by_xpath('//div[@class=\"product-price\"]')  #locating web element of title\n",
    "    for i in price_tags:                       #iterating over each web element of title\n",
    "        Price=i.text                #extracting text from each web element\n",
    "        price.append(Price)    #appending each extracted text in the empty list\n",
    "    #code to click on next button\n",
    "    next_button=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[2]/div/div[2]/section/div[2]/ul/li[2]/a\")\n",
    "    next_button.click()\n",
    "    time.sleep(5)  \n",
    "price\n",
    "Pr=price[:100]\n",
    "print(Pr)\n",
    "\n",
    "#check Length\n",
    "print(len(B),len(Pr),len(P))\n",
    "\n",
    "#Make DataFrame\n",
    "DF=pd.DataFrame({\"Brand\":B,\"Product Description\":P,\"Price\":Pr})\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59f18564",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a29ce8e",
   "metadata": {},
   "source": [
    "# Q8: Go to webpage https://www.amazon.in/\n",
    "        Scrape 1. Title, 2. rating 3. price for first 10 laptops\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "63e06eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to webdriver:\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\jyuth\\chrome driver\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "#opening a website on a chrome window\n",
    "driver.get('https://www.amazon.in/')\n",
    "\n",
    "#finding element for laptop in search bar\n",
    "search_field_designation=driver.find_element_by_id(\"twotabsearchtextbox\") #Laptop in search bar\n",
    "search_field_designation.send_keys(\"Laptop\")\n",
    "\n",
    "#clicking on search button\n",
    "search_button=driver.find_element_by_id('nav-search-submit-button')\n",
    "search_button.click()\n",
    "\n",
    "#clicking on search box for i7 (i9 was not displayed in the options on the webpage)\n",
    "search_box=driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[6]/ul[4]/li[14]/span/a/span')\n",
    "search_box.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "99cd43b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acer Predator Helios 300 11th Gen Intel Core i...</td>\n",
       "      <td>1,69,990</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSI Modern 14, Intel i7-1195G7, 14\" FHD IPS-Le...</td>\n",
       "      <td>76,490</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mi Notebook Ultra 3.2K Resolution Display Inte...</td>\n",
       "      <td>77,990</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Samsung Galaxy Book2 Intel 12th Gen core i7 Ev...</td>\n",
       "      <td>79,990</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....</td>\n",
       "      <td>86,990</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HP Pavilion 14, 11th Gen Intel Core i7-16GB RA...</td>\n",
       "      <td>86,990</td>\n",
       "      <td>4.6 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...</td>\n",
       "      <td>89,990</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...</td>\n",
       "      <td>57,490</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LG Gram Intel Evo 11th Gen Core i7 17 inches U...</td>\n",
       "      <td>93,999</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HP Pavilion x360 11th Gen Intel Core i7 14 inc...</td>\n",
       "      <td>85,890</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title     Price  \\\n",
       "0  Acer Predator Helios 300 11th Gen Intel Core i...  1,69,990   \n",
       "1  MSI Modern 14, Intel i7-1195G7, 14\" FHD IPS-Le...    76,490   \n",
       "2  Mi Notebook Ultra 3.2K Resolution Display Inte...    77,990   \n",
       "3  Samsung Galaxy Book2 Intel 12th Gen core i7 Ev...    79,990   \n",
       "4  Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....    86,990   \n",
       "5  HP Pavilion 14, 11th Gen Intel Core i7-16GB RA...    86,990   \n",
       "6  ASUS TUF Gaming F15 (2021), 15.6\" (39.62 cms) ...    89,990   \n",
       "7  ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...    57,490   \n",
       "8  LG Gram Intel Evo 11th Gen Core i7 17 inches U...    93,999   \n",
       "9  HP Pavilion x360 11th Gen Intel Core i7 14 inc...    85,890   \n",
       "\n",
       "               Rating  \n",
       "0  5.0 out of 5 stars  \n",
       "1  4.0 out of 5 stars  \n",
       "2  4.3 out of 5 stars  \n",
       "3  4.5 out of 5 stars  \n",
       "4  4.4 out of 5 stars  \n",
       "5  4.6 out of 5 stars  \n",
       "6  4.5 out of 5 stars  \n",
       "7  4.5 out of 5 stars  \n",
       "8  4.0 out of 5 stars  \n",
       "9  4.3 out of 5 stars  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create 3 empty lists which needs to be extracted\n",
    "Title=[]\n",
    "Price=[]\n",
    "Rating=[]\n",
    "\n",
    "#find title\n",
    "title_tags=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\") #locating web elements for the laptop\n",
    "\n",
    "#we have all tags for the job title. We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in title_tags:            #iterating over web element of title\n",
    "    title=i.text                #extracting text from each web element\n",
    "    Title.append(title)         #appending each extracted text in the empty list\n",
    "T=Title[:10]                    #printing top 10 data \n",
    "T                    \n",
    "\n",
    "#Extracting the html tags for the price\n",
    "price_tags=driver.find_elements_by_xpath('//span[@class=\"a-price-whole\"]')\n",
    "\n",
    "#we have all tags for the company. We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in price_tags:            #iterating over web element of title\n",
    "    price=i.text                #extracting text from each web element\n",
    "    Price.append(price)         #appending each extracted text in the empty list\n",
    "P=Price[:10]\n",
    "P\n",
    "\n",
    "#Extracting the html tags for the ratings\n",
    "r_tags=driver.find_elements_by_xpath('//div[@class=\"a-row a-size-small\"]/span[1]')\n",
    "\n",
    "#we have all tags for the experience. We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in r_tags:                               #iterating over web element of title\n",
    "    r=i.get_attribute('aria-label')            #extracting text from each web element\n",
    "    Rating.append(r)                           #appending each extracted text in the empty list\n",
    "R=Rating[:10]\n",
    "R \n",
    "\n",
    "#Check the length\n",
    "print(len(T),len(P),len(R))\n",
    "\n",
    "#make data frame\n",
    "df=pd.DataFrame({'Title':T,'Price':P,'Rating':R})\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6bacb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a03594",
   "metadata": {},
   "source": [
    "# Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida location in https://www.ambitionbox.com/. \n",
    "    You have to scrape company name, No. of days ago when job was posted, Rating of the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19acb58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Job posted</th>\n",
       "      <th>Company Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EXL Services.com ( I ) Pvt. Ltd.</td>\n",
       "      <td>8d ago</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>20d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TECHNIP GLOBAL BUSINESS SERVICES PRIVATE LIMITED</td>\n",
       "      <td>6d ago</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>1mon ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bristlecone India Limited</td>\n",
       "      <td>15d ago</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zyoin</td>\n",
       "      <td>19d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ashkom Media India Private Limited</td>\n",
       "      <td>6d ago</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Newgen Software Technologies Ltd.</td>\n",
       "      <td>21d ago</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JK Technosoft Ltd</td>\n",
       "      <td>30d ago</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pitney Bowes India Pvt Ltd</td>\n",
       "      <td>1mon ago</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Company Name Job posted Company Rating\n",
       "0                  EXL Services.com ( I ) Pvt. Ltd.     8d ago            3.9\n",
       "1                     GENPACT India Private Limited    20d ago            4.0\n",
       "2  TECHNIP GLOBAL BUSINESS SERVICES PRIVATE LIMITED     6d ago            3.9\n",
       "3                     GENPACT India Private Limited   1mon ago            4.0\n",
       "4                         Bristlecone India Limited    15d ago            3.8\n",
       "5                                             Zyoin    19d ago            4.1\n",
       "6                Ashkom Media India Private Limited     6d ago            3.7\n",
       "7                 Newgen Software Technologies Ltd.    21d ago            3.5\n",
       "8                                 JK Technosoft Ltd    30d ago            3.7\n",
       "9                        Pitney Bowes India Pvt Ltd   1mon ago            4.2"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#connect to webdriver:\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\jyuth\\chrome driver\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "#opening a website on a chrome window\n",
    "driver.get('https://www.ambitionbox.com/')\n",
    "\n",
    "#selecting the \"job\" tab\n",
    "tab_search=driver.find_element_by_xpath(\"/html/body/div[1]/nav/nav/a[6]\")\n",
    "tab_search.click()\n",
    "\n",
    "#finding element for data scientist in search bar\n",
    "search_field_designation=driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/div/span/input\")\n",
    "search_field_designation.send_keys(\"Data Scientist\")\n",
    "\n",
    "#searching the results using search button\n",
    "search=driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/button/span')\n",
    "search.click()\n",
    "\n",
    "#create 3 empty lists which needs to be extracted\n",
    "CN=[]\n",
    "NosD=[]\n",
    "Rating=[]\n",
    "\n",
    "#find Company name\n",
    "CN_tags=driver.find_elements_by_xpath(\"//p[@class='company body-medium']\") #locating web elements for the company name\n",
    "\n",
    "#we have all tags, We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in CN_tags:            #iterating over web element of title\n",
    "    C=i.text                 #extracting text from each web element\n",
    "    CN.append(C)             #appending each extracted text in the empty list\n",
    "print(CN[:10])               #printing top 10 data        \n",
    "\n",
    "#Extracting the html tags for the nos of days ago it was posted\n",
    "D_tags=driver.find_elements_by_xpath('//span[@class=\"body-small-l\"]')\n",
    "\n",
    "#we have all tags for the nos of days. We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in D_tags:            #iterating over web element of title\n",
    "    d=i.text                #extracting text from each web element\n",
    "    NosD.append(d)          #appending each extracted text in the empty list\n",
    "ND=NosD[0::2]                #selecting the odd results to get the required data\n",
    "N=ND[:10]\n",
    "N\n",
    "\n",
    "#find Comapnay ratings\n",
    "R_tags=driver.find_elements_by_xpath('//span[@class=\"body-small\"]') #locating web elements for the rating title\n",
    "\n",
    "#we have all tags, We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in R_tags:            #iterating over web element of title\n",
    "    R=i.text                #extracting text from each web element\n",
    "    Rating.append(R)        #appending each extracted text in the empty list\n",
    "print(Rating[:10])          #printing top 10 data        \n",
    "\n",
    "print(len(CN),len(N),len(Rating)) #checking length of lists\n",
    "\n",
    "#Make DF\n",
    "df1=pd.DataFrame({\"Company Name\": CN,\"Job posted\":N,\"Company Rating\":Rating})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a8c7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda15380",
   "metadata": {},
   "source": [
    "# Q10: Write a python program to scrape the salary data for Data Scientist designation from https://www.ambitionbox.com/\n",
    "You have to scrape 1. Company name, 2. Number of salaries, 3. Average salary, 4. Minsalary, 5. Max Salary, 6. Experienec needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8290486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to webdriver:\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\jyuth\\chrome driver\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "#opening a website on a chrome window\n",
    "driver.get('https://www.ambitionbox.com/')\n",
    "\n",
    "#find element for \"Salary\" in the tab\n",
    "tab_find=driver.find_element_by_xpath(\"/html/body/div[1]/nav/nav/a[4]\")\n",
    "tab_find.click()\n",
    "\n",
    "#finding element for data scientist in search bar\n",
    "search_field_designation=driver.find_element_by_xpath(\"/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/input\")\n",
    "search_field_designation.send_keys(\"Data Scientist\")\n",
    "\n",
    "#searching the results from drop down menu\n",
    "search=driver.find_element_by_xpath(\"/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/div/div/div[1]\")\n",
    "search.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae430135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name and salaries</th>\n",
       "      <th>Experience needed</th>\n",
       "      <th>Average Salary</th>\n",
       "      <th>Minimum Salary</th>\n",
       "      <th>Maximun Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart\\nbased on 11 salaries</td>\n",
       "      <td>3 yrs exp</td>\n",
       "      <td>₹ 29.7L</td>\n",
       "      <td>₹ 25.0L</td>\n",
       "      <td>₹ 35.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ab Inbev\\nbased on 32 salaries</td>\n",
       "      <td>3-4 yrs exp</td>\n",
       "      <td>₹ 20.5L</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>₹ 25.5L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reliance Jio\\nbased on 10 salaries</td>\n",
       "      <td>4 yrs exp</td>\n",
       "      <td>₹ 18.9L</td>\n",
       "      <td>₹ 5.6L</td>\n",
       "      <td>₹ 26.2L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZS\\nbased on 15 salaries</td>\n",
       "      <td>2 yrs exp</td>\n",
       "      <td>₹ 15.9L</td>\n",
       "      <td>₹ 9.8L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optum\\nbased on 27 salaries</td>\n",
       "      <td>3-4 yrs exp</td>\n",
       "      <td>₹ 15.2L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 22.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fractal Analytics\\nbased on 81 salaries</td>\n",
       "      <td>2-4 yrs exp</td>\n",
       "      <td>₹ 15.2L</td>\n",
       "      <td>₹ 9.5L</td>\n",
       "      <td>₹ 22.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tiger Analytics\\nbased on 46 salaries</td>\n",
       "      <td>2-4 yrs exp</td>\n",
       "      <td>₹ 14.8L</td>\n",
       "      <td>₹ 9.0L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UnitedHealth\\nbased on 53 salaries</td>\n",
       "      <td>2-4 yrs exp</td>\n",
       "      <td>₹ 14.0L</td>\n",
       "      <td>₹ 8.3L</td>\n",
       "      <td>₹ 20.5L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Verizon\\nbased on 14 salaries</td>\n",
       "      <td>4 yrs exp</td>\n",
       "      <td>₹ 12.7L</td>\n",
       "      <td>₹ 10.0L</td>\n",
       "      <td>₹ 21.0L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ganit Business Solutions\\nbased on 13 salaries</td>\n",
       "      <td>4 yrs exp</td>\n",
       "      <td>₹ 12.4L</td>\n",
       "      <td>₹ 8.5L</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Company Name and salaries Experience needed  \\\n",
       "0                   Walmart\\nbased on 11 salaries         3 yrs exp   \n",
       "1                  Ab Inbev\\nbased on 32 salaries       3-4 yrs exp   \n",
       "2              Reliance Jio\\nbased on 10 salaries         4 yrs exp   \n",
       "3                        ZS\\nbased on 15 salaries         2 yrs exp   \n",
       "4                     Optum\\nbased on 27 salaries       3-4 yrs exp   \n",
       "5         Fractal Analytics\\nbased on 81 salaries       2-4 yrs exp   \n",
       "6           Tiger Analytics\\nbased on 46 salaries       2-4 yrs exp   \n",
       "7              UnitedHealth\\nbased on 53 salaries       2-4 yrs exp   \n",
       "8                   Verizon\\nbased on 14 salaries         4 yrs exp   \n",
       "9  Ganit Business Solutions\\nbased on 13 salaries         4 yrs exp   \n",
       "\n",
       "  Average Salary Minimum Salary Maximun Salary  \n",
       "0        ₹ 29.7L        ₹ 25.0L        ₹ 35.0L  \n",
       "1        ₹ 20.5L        ₹ 15.0L        ₹ 25.5L  \n",
       "2        ₹ 18.9L         ₹ 5.6L        ₹ 26.2L  \n",
       "3        ₹ 15.9L         ₹ 9.8L        ₹ 20.0L  \n",
       "4        ₹ 15.2L        ₹ 11.0L        ₹ 22.0L  \n",
       "5        ₹ 15.2L         ₹ 9.5L        ₹ 22.0L  \n",
       "6        ₹ 14.8L         ₹ 9.0L        ₹ 20.0L  \n",
       "7        ₹ 14.0L         ₹ 8.3L        ₹ 20.5L  \n",
       "8        ₹ 12.7L        ₹ 10.0L        ₹ 21.0L  \n",
       "9        ₹ 12.4L         ₹ 8.5L        ₹ 15.0L  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create 6 empty lists which needs to be extracted\n",
    "CN=[]\n",
    "NoS=[]\n",
    "AvgS=[]\n",
    "MinS=[]\n",
    "MaxS=[]\n",
    "Exp=[]\n",
    "\n",
    "#find Company name\n",
    "CN_tags=driver.find_elements_by_xpath('//div[@class=\"name\"]') #locating web elements for company name\n",
    "\n",
    "#we have all tags, We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in CN_tags:            #iterating over web element of title\n",
    "    C=i.text                #extracting text from each web element\n",
    "    CN.append(C)           #appending each extracted text in the empty list\n",
    "CN\n",
    "\n",
    "#find Avg Salary\n",
    "Av_tags=driver.find_elements_by_xpath('//p[@class=\"averageCtc\"]') #locating web elements\n",
    "Av_tags                    \n",
    "\n",
    "#we have all tags, We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in Av_tags:            #iterating over web element of title\n",
    "    A=i.text                 #extracting text from each web element\n",
    "    AvgS.append(A)           #appending each extracted text in the empty list\n",
    "a=AvgS[:10]                  #selecting first 10 data\n",
    "a\n",
    "\n",
    "#find Min Salary\n",
    "Mi_tags=driver.find_elements_by_xpath('//div[@class=\"value body-medium\"]') #locating web elements\n",
    "Mi_tags                    \n",
    "\n",
    "#we have all tags, We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in Mi_tags:            #iterating over web element of title\n",
    "    m=i.text                 #extracting text from each web element\n",
    "    MinS.append(m)           #appending each extracted text in the empty list\n",
    "MinSal=MinS[::2][:10]         #printing top 10 data, with jump of 1 to get needed result and storing in new variable\n",
    "MinSal\n",
    "\n",
    "#find Max Salary\n",
    "Ma_tags=driver.find_elements_by_xpath('//div[@class=\"value body-medium\"]') #locating web elements\n",
    "Ma_tags                    \n",
    "\n",
    "#we have all tags, We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in Ma_tags:            #iterating over web element of title\n",
    "    M=i.text                 #extracting text from each web element\n",
    "    MaxS.append(M)           #appending each extracted text in the empty list\n",
    "ma=MaxS[1:20:2]             #select element from 1 to 20 and jump the 2 elements in between\n",
    "ma\n",
    "\n",
    "#find Experience\n",
    "e_tags=driver.find_elements_by_xpath('//div[@class=\"salaries sbold-list-header\"]') #locating web elements\n",
    "e_tags\n",
    "\n",
    "#we have all tags, We need to extract text from these tag. for that we need to run a loop.\n",
    "for i in e_tags:                                  #iterating over web element of title\n",
    "    E=i.text.replace(\"Data Scientist\\n . \\n\",\"\")  #extracting text from each web element\n",
    "    Exp.append(E)                                 #appending each extracted text in the empty list\n",
    "Exp[:10]                                           #listing first 10 data\n",
    "\n",
    "print(len(CN),len(a),len(MinSal),len(ma),len(Exp)) #Checking length of the lists\n",
    "\n",
    "#make DF\n",
    "df2=pd.DataFrame({\"Company Name and salaries\":CN,\"Experience needed\":Exp,\"Average Salary\":a,\"Minimum Salary\":MinSal,\"Maximun Salary\":ma})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd700412",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1bc6f",
   "metadata": {},
   "source": [
    "# END OF THE ASSIGNMENT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
